{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import cv2\n",
    "import math\n",
    "from glob import glob\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "from src.modeling.sharingan import Sharingan\n",
    "from src.utils.common import spatial_argmax2d, square_bbox\n",
    "\n",
    "%config InlineBackend.figure_format = \"retina\"\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Globals\n",
    "det_thr = 0.4\n",
    "img_mean = [0.44232, 0.40506, 0.36457]\n",
    "img_std = [0.28674, 0.27776, 0.27995]\n",
    "\n",
    "COLOR_NAMES = [\"mediumvioletred\", \"green\", \"dodgerblue\", \"crimson\", \"goldenrod\", \n",
    "               \"DarkSlateGray\", \"saddlebrown\", \"purple\", \"teal\"]\n",
    "COLORS = [(199, 21, 133), (0, 128, 0), (30, 144, 255), (220, 20, 60), (218, 165, 32), \n",
    "          (47, 79, 79), (139, 69, 19), (128, 0, 128), (0, 128, 128)]\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_params(module):\n",
    "    return sum([param.numel() for param in module.parameters()])\n",
    "\n",
    "\n",
    "def expand_bbox(bbox, img_w, img_h, k=0.1):\n",
    "    w, h = bbox[2] - bbox[0], bbox[3] - bbox[1]\n",
    "    bbox[0] = max(0, bbox[0] - k * w)\n",
    "    bbox[1] = max(0, bbox[1] - k * h)\n",
    "    bbox[2] = min(img_w, bbox[2] + k * w)\n",
    "    bbox[3] = min(img_h, bbox[3] + k * h)\n",
    "    return bbox\n",
    "\n",
    "\n",
    "def load_head_detection_model(device):\n",
    "\t# Load and return the pre-trained head detection model\n",
    "\tckpt_path = \"./weights/yolov5m_crowdhuman.pt\"\n",
    "\tmodel = torch.hub.load(\"ultralytics/yolov5\", \"custom\", path=ckpt_path, verbose=False)\n",
    "\tmodel.conf = 0.25  # NMS confidence threshold\n",
    "\tmodel.iou = 0.45  # NMS IoU threshold\n",
    "\tmodel.classes = [1]  # filter by class, i.e. = [1] for heads\n",
    "\tmodel.amp = False  # Automatic Mixed Precision (AMP) inference\n",
    "\tmodel = model.to(device)\n",
    "\tmodel.eval()\n",
    "\treturn model\n",
    "\n",
    "\n",
    "def detect_heads(image, model):\n",
    "\t\"\"\"\n",
    "\tDetect heads in the image using the provided model.\n",
    "\tReturns a numpy array containing the detected head bboxes and their confidence scores.\n",
    "\t\"\"\"\n",
    "\tdetections = model(image, size=640).pred[0].numpy()[:, :-1] # filter out the class column\n",
    "\treturn detections\n",
    "\n",
    "\n",
    "def load_sharingan_model(ckpt_path, device):\n",
    "    # Build model\n",
    "\tsharingan = Sharingan(\n",
    "\t\tpatch_size=16,\n",
    "\t\ttoken_dim=768,\n",
    "\t\timage_size=224,\n",
    "\t\tgaze_feature_dim=512,\n",
    "\t\tencoder_depth=12,\n",
    "\t\tencoder_num_heads=12,\n",
    "\t\tencoder_num_global_tokens=0,\n",
    "\t\tencoder_mlp_ratio=4.0,\n",
    "\t\tencoder_use_qkv_bias=True,\n",
    "\t\tencoder_drop_rate=0.0,\n",
    "\t\tencoder_attn_drop_rate=0.0,\n",
    "\t\tencoder_drop_path_rate=0.0,\n",
    "\t\tdecoder_feature_dim=128,\n",
    "\t\tdecoder_hooks=[2, 5, 8, 11],\n",
    "\t\tdecoder_hidden_dims=[48, 96, 192, 384],\n",
    "\t\tdecoder_use_bn=True,\n",
    "\t)\n",
    "\n",
    "\t# Load checkpoint\n",
    "\tcheckpoint = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "\tcheckpoint = {name.replace(\"model.\", \"\"): value for name, value in checkpoint[\"state_dict\"].items()}\n",
    "\tsharingan.load_state_dict(checkpoint, strict=True)\n",
    "\tsharingan.eval()\n",
    "\tsharingan.to(device)\n",
    "\treturn sharingan\n",
    "\n",
    "\n",
    "def predict_gaze(img_path, sharingan, head_detector, det_thr=0.4):\n",
    "\t# 1. Read image\n",
    "\timage = Image.open(img_path).convert('RGB')\n",
    "\timage_np = np.array(image)\n",
    "\timg_h, img_w, img_c = image_np.shape\n",
    "\n",
    "\t# 2. detect & process head bboxes\n",
    "\tdetections = detect_heads(image_np, head_detector)\n",
    "\thead_bboxes = []\n",
    "\tfor detection in detections:\n",
    "\t\tbbox, conf = detection[:4], detection[4]\n",
    "\t\tif conf > det_thr:\n",
    "\t\t\thead_bboxes.append(bbox)  \n",
    "\thead_bboxes = torch.tensor(np.stack(head_bboxes))\n",
    "\tt_head_bboxes = square_bbox(head_bboxes, img_w, img_h)\n",
    "\n",
    "\tnum_heads = len(head_bboxes)\n",
    "\tprint(f\"Detected {num_heads} heads.\")\n",
    "\n",
    "\t# 3. Extract and transform heads\n",
    "\theads = []\n",
    "\tfor bbox in t_head_bboxes:\n",
    "\t\thead = TF.resize(TF.to_tensor(image.crop(bbox.numpy())), (224, 224))\n",
    "\t\theads.append(head)\n",
    "\theads = torch.stack(heads)\n",
    "\theads = TF.normalize(heads, mean=img_mean, std=img_std)\n",
    "\n",
    "\t# 4. Transform Image\n",
    "\timage = TF.to_tensor(image)\n",
    "\timage = TF.resize(image, (224, 224))\n",
    "\timage = TF.normalize(image, mean=img_mean, std=img_std)\n",
    "\n",
    "\t# 5. Normalize head bboxes\n",
    "\tscale = torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32)\n",
    "\tt_head_bboxes /= scale\n",
    "\n",
    "\t# 6. build input sample\n",
    "\tsample = {}\n",
    "\tsample[\"image\"] = image.unsqueeze(0) # (1, 3, 224, 224)\n",
    "\tsample[\"heads\"] = heads.unsqueeze(0) # (1, num_heads, 3, 224, 224)\n",
    "\tsample[\"head_bboxes\"] = t_head_bboxes.unsqueeze(0) # (1, num_heads, 4)\n",
    "\n",
    "\t# 7. predict gaze\n",
    "\twith torch.no_grad():\n",
    "\t\tgaze_vecs, gaze_heatmaps, inouts = sharingan(sample)\n",
    "\t\tgaze_heatmaps = gaze_heatmaps.squeeze(0)\n",
    "\t\tgaze_vecs = gaze_vecs.squeeze(0)\n",
    "\t\tgaze_points = spatial_argmax2d(gaze_heatmaps, normalize=True)\n",
    "\t\tinouts = torch.sigmoid(inouts.squeeze(0)).flatten()\n",
    "  \n",
    "\treturn image_np, head_bboxes, gaze_points, gaze_vecs, inouts, gaze_heatmaps\n",
    "\n",
    "def draw_gaze(\n",
    "    image,\n",
    "    head_bboxes,\n",
    "    gaze_points,\n",
    "    gaze_vecs,\n",
    "    inouts,\n",
    "    pids,\n",
    "    gaze_heatmaps,\n",
    "    heatmap_pid = None,\n",
    "    frame_nb = None,\n",
    "    colors = COLORS,\n",
    "    alpha: float = 0.5,\n",
    "    io_thr: float = 0.4, \n",
    "    gaze_pt_size: int = 10,\n",
    "    gaze_vec_factor: float = 0.8,\n",
    "    head_center_size: int = 10,\n",
    "    thickness: int = 4,\n",
    "    fs: float = 0.6,\n",
    "):\n",
    "    \"\"\"\n",
    "\tDraws gaze results on the given image.\n",
    " \n",
    "\tArgs:\n",
    "\t\timage (np.ndarray): The input image on which to draw.\n",
    "\t\thead_bboxes (array-like): Bounding boxes for heads.\n",
    "\t\tgaze_points (array-like): Points representing gaze locations.\n",
    "\t\tgaze_vecs (array-like): Vectors representing gaze directions.\n",
    "\t\tinouts (array-like): In/out scores for each head.\n",
    "\t\tpids (array-like): Person IDs for each head.\n",
    "\t\tgaze_heatmaps (array-like): Heatmaps for gaze.\n",
    "\t\theatmap_pid (int, optional): Person ID for which to draw the heatmap. Defaults to None.\n",
    "\t\tframe_nb (int, optional): Frame number to display on the image. Defaults to None.\n",
    "\t\tcolors (array-like, optional): Colors to use for drawing. Defaults to COLORS.\n",
    "\t\talpha (float, optional): Alpha blending value for heatmap overlay. Defaults to 0.5.\n",
    "\t\tio_thr (float, optional): Threshold for in/out scores to draw gaze points. Defaults to 0.5.\n",
    "\t\tgaze_pt_size (int, optional): Size of the gaze points. Defaults to 10.\n",
    "\t\tgaze_vec_factor (float, optional): Scaling factor for gaze vectors. Defaults to 0.8.\n",
    "\t\thead_center_size (int, optional): Size of the head center points. Defaults to 10.\n",
    "\t\tthickness (int, optional): Thickness of the drawing lines. Defaults to 4.\n",
    "\t\tfs (float, optional): Font scale for text. Defaults to 0.6.\n",
    "\tReturns:\n",
    "\t\tnp.ndarray: The image with gaze results drawn on it.\n",
    "    \"\"\"\n",
    "    # Create canvas on which to draw predictions\n",
    "    img_h, img_w, img_c = image.shape\n",
    "    canvas = image.copy()\n",
    "    \n",
    "    # Scale of the drawing according to image resolution\n",
    "    scale = max(img_h, img_w) / 1920\n",
    "    fs *= scale\n",
    "    thickness = int(scale * thickness)\n",
    "    gaze_pt_size = int(scale * gaze_pt_size)\n",
    "    head_center_size = int(scale * head_center_size)\n",
    "    \n",
    "    # Draw heatmap\n",
    "    if heatmap_pid is not None:\n",
    "        if len(gaze_heatmaps) == 0:\n",
    "            raise ValueError(\"gaze_heatmaps must be provided if heatmap_pid is provided.\")\n",
    "        mask = (pids == heatmap_pid)\n",
    "        if mask.sum() == 1: # only if detection found\n",
    "            gaze_heatmap = gaze_heatmaps[mask]\n",
    "            heatmap = TF.resize(gaze_heatmap, (img_h, img_w), antialias=True).squeeze().numpy()\n",
    "            heatmap = (heatmap - heatmap.min()) / (heatmap.max() - heatmap.min())\n",
    "            heatmap = cm.inferno(heatmap) * 255 \n",
    "            canvas = ((1 - alpha) * image + alpha * heatmap[..., :3]).astype(np.uint8)\n",
    "\n",
    "            # Write pid being used for the heatmap\n",
    "            hm_pid_text = f\"Heatmap PID: {heatmap_pid}\"\n",
    "            (w_text, h_text), _ = cv2.getTextSize(hm_pid_text, cv2.FONT_HERSHEY_SIMPLEX, fs, 1)\n",
    "            ul = (img_w - w_text - 20, img_h - h_text - 15)\n",
    "            br = (img_w, img_h)\n",
    "            cv2.rectangle(canvas, ul, br, (0, 0, 0), -1)\n",
    "            hm_pid_text_loc = (img_w - w_text - 10, img_h - 10)\n",
    "            cv2.putText(canvas, hm_pid_text, hm_pid_text_loc, cv2.FONT_HERSHEY_SIMPLEX, fs, (255, 255, 255), 1, cv2.LINE_AA)   \n",
    "\n",
    "    # Draw head bboxes  \n",
    "    if len(head_bboxes) > 0:\n",
    "        if len(pids) == 0:\n",
    "            raise ValueError(\"pids must be provided if head_bboxes is provided.\")\n",
    "        \n",
    "        # Convert to numpy\n",
    "        head_bboxes = head_bboxes.numpy() if isinstance(head_bboxes, torch.Tensor) else np.array(head_bboxes)\n",
    "        inouts = inouts.numpy() if isinstance(inouts, torch.Tensor) else np.array(inouts)\n",
    "        if head_bboxes.max() <= 1.0:\n",
    "            head_bboxes = head_bboxes * np.array([img_w, img_h, img_w, img_h])\n",
    "        head_bboxes = head_bboxes.astype(int)\n",
    "        \n",
    "        # Compute head center\n",
    "        head_centers = np.hstack([(head_bboxes[:,[0]] + head_bboxes[:,[2]]) / 2, (head_bboxes[:,[1]] + head_bboxes[:,[3]]) / 2])\n",
    "        head_centers = head_centers.astype(int)\n",
    "        \n",
    "        gaze_available = (len(gaze_points) > 0)\n",
    "        if gaze_available and (len(inouts) == 0):\n",
    "            raise ValueError(\"inouts must be provided if gaze_pts is provided.\")\n",
    "            \n",
    "        if gaze_available:\n",
    "            gaze_points = gaze_points.numpy() if isinstance(gaze_points, torch.Tensor) else np.array(gaze_points)\n",
    "            if (gaze_points.max() <= 1.):\n",
    "                gaze_points = gaze_points * np.array([img_w, img_h])\n",
    "            gaze_points = gaze_points.astype(int)\n",
    "            \n",
    "        if gaze_vecs is not None:\n",
    "            gaze_vecs = gaze_vecs.numpy() if isinstance(gaze_vecs, torch.Tensor) else np.array(gaze_vecs)\n",
    "        \n",
    "        for i, head_bbox in enumerate(head_bboxes):\n",
    "            \n",
    "            if (heatmap_pid is not None) and (heatmap_pid != i):\n",
    "                continue\n",
    "            \n",
    "            xmin, ymin, xmax, ymax = head_bbox\n",
    "            head_radius = max(xmax-xmin, ymax-ymin) // 2\n",
    "            pid = pids[i]\n",
    "            color = colors[pid % len(colors)]\n",
    "                            \n",
    "            # Compute Head Center\n",
    "            head_center = head_centers[i]\n",
    "        \n",
    "            head_bbox_ul = (xmin, ymin)\n",
    "            head_bbox_br = (xmax, ymax)\n",
    "            head_center_ul = head_center - (head_center_size // 2)\n",
    "            head_center_br = head_center + (head_center_size // 2)\n",
    "            cv2.rectangle(canvas, head_center_ul, head_center_br, color, -1) # head center point\n",
    "            cv2.circle(canvas, head_center, head_radius, color, thickness) # head circle\n",
    "            \n",
    "            # Draw header\n",
    "            io = inouts[i] if inouts is not None else \"-\"\n",
    "            header_text = f\"P{pid}: {io:.2f}\"\n",
    "            (w_text, h_text), _ = cv2.getTextSize(header_text, cv2.FONT_HERSHEY_SIMPLEX, fs, 1)\n",
    "            \n",
    "            header_ul =  (int(head_center[0] - w_text / 2), int(ymin - thickness / 2))\n",
    "            header_br = (int(head_center[0] + w_text / 2), int(ymin + h_text + 5))\n",
    "            cv2.rectangle(canvas, header_ul, header_br, color, -1) # header bbox\n",
    "            cv2.putText(canvas, header_text, (header_ul[0], int(ymin + h_text)), cv2.FONT_HERSHEY_SIMPLEX, fs, (255, 255, 255), 1, cv2.LINE_AA) # header text\n",
    "            \n",
    "            if gaze_available and (io > io_thr):\n",
    "                gp = gaze_points[i]\n",
    "                vec = (gp - head_center)\n",
    "                vec = vec / (np.linalg.norm(vec) + 0.000001)\n",
    "                intersection = head_center + (vec * head_radius).astype(int)\n",
    "                #cv2.line(canvas, head_center, gp, color, int(0.5 * thickness)) # UNCOMMENT\n",
    "                cv2.line(canvas, intersection, gp, color, thickness)\n",
    "                \n",
    "                cv2.circle(canvas, gp, gaze_pt_size, color, -1)\n",
    "                \n",
    "            if gaze_vecs is not None:\n",
    "                gv = gaze_vecs[i]\n",
    "                cv2.arrowedLine(canvas, head_center, (head_center + gaze_vec_factor * head_radius * gv).astype(int), color, thickness)\n",
    "                \n",
    "                \n",
    "    # Write frame number\n",
    "    if frame_nb is not None:\n",
    "        frame_nb = str(frame_nb)\n",
    "        (w_text, h_text), _ = cv2.getTextSize(frame_nb, cv2.FONT_HERSHEY_SIMPLEX, fs, 1)\n",
    "        nb_ul = (int((img_w - w_text) / 2), (img_h - h_text - 15))\n",
    "        nb_br = (int((img_w + w_text) / 2), img_h)\n",
    "        cv2.rectangle(canvas, nb_ul, nb_br, (0, 0, 0), -1)\n",
    "        nb_text_loc = (int((img_w - w_text) / 2), (img_h - 10))\n",
    "        cv2.putText(canvas, frame_nb, nb_text_loc, cv2.FONT_HERSHEY_SIMPLEX, fs, (255, 255, 255), 1, cv2.LINE_AA) \n",
    "\n",
    "    return canvas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Head Detector\n",
    "head_detector = load_head_detection_model(device)\n",
    "head_detector_num_params = get_num_params(head_detector)\n",
    "head_detector_num_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Sharingan model\n",
    "#ckpt_path = \"checkpoints/gazefollow.pt\"\n",
    "#ckpt_path = \"checkpoints/childplay.pt\"\n",
    "ckpt_path = \"checkpoints/videoattentiontarget.pt\"\n",
    "\n",
    "sharingan = load_sharingan_model(ckpt_path, device)\n",
    "sharingan_num_params = get_num_params(sharingan)\n",
    "sharingan_num_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and execute this cell once so matplotlib will display the plots inline\n",
    "#%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict gaze for a given image\n",
    "img_path = \"samples/image1.jpg\" # change to the path of the image you want to test e.g. \"data/image2.jpg\" or \"data/image3.jpg\"\n",
    "output = predict_gaze(img_path, sharingan, head_detector, det_thr=det_thr)\n",
    "image_np, head_bboxes, gaze_points, gaze_vecs, inouts, gaze_heatmaps = output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize\n",
    "show_gaze_vec = True\n",
    "alpha = 0.7\n",
    "fs = 1.\n",
    "thickness = 10\n",
    "gaze_pt_size = 20\n",
    "head_center_size = 18\n",
    "gaze_vec_factor = 0.6\n",
    "\n",
    "img_h, img_w = image_np.shape[:2]\n",
    "num_people = len(head_bboxes)\n",
    "pids = np.arange(num_people)\n",
    "\n",
    "num_axes = 2 + num_people\n",
    "ncols = 2\n",
    "nrows = np.ceil(num_axes / ncols).astype(int)\n",
    "fig_w = 20\n",
    "ax_w = fig_w // ncols\n",
    "ax_h = int(round(ax_w * img_h / img_w))\n",
    "\n",
    "fig, axes = plt.subplots(figsize = (fig_w, ax_h * nrows), nrows = nrows, ncols = ncols, tight_layout=True)\n",
    "axes = axes.flatten()\n",
    "[ax.axis(\"off\") for ax in axes]\n",
    "\n",
    "# Show input image\n",
    "axes[0].imshow(image_np)\n",
    "\n",
    "# Iterate over people and show the heatmap of each. The first iteration (ie. None) shows all predictions without heatmaps\n",
    "i = 1\n",
    "for heatmap_pid in [None] + np.arange(num_people).tolist():\n",
    "    frame = draw_gaze(image_np, \n",
    "                      head_bboxes = head_bboxes, \n",
    "                      gaze_points = gaze_points, \n",
    "                      gaze_vecs = gaze_vecs if show_gaze_vec else None, \n",
    "                      inouts = inouts, \n",
    "                      pids = pids, \n",
    "                      gaze_heatmaps = gaze_heatmaps, \n",
    "                      heatmap_pid = heatmap_pid, \n",
    "                      frame_nb = None, \n",
    "                      colors = COLORS,\n",
    "                      alpha = alpha, \n",
    "                      gaze_pt_size = gaze_pt_size,\n",
    "                      gaze_vec_factor = gaze_vec_factor,\n",
    "                      head_center_size = head_center_size,\n",
    "                      thickness = thickness,\n",
    "                      fs = fs,\n",
    "                     ) \n",
    "\n",
    "    axes[i].imshow(frame)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sharingan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
